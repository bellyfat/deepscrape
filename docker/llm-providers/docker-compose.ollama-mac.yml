version: '3.8'

services:
  # Ollama - Local LLM runner (macOS version without GPU)
  ollama:
    image: ollama/ollama:latest
    container_name: deepscrape-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      # Ollama configuration
      - OLLAMA_MODELS=${OLLAMA_MODELS_PATH:-/root/.ollama/models}
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_NUM_PARALLEL=4
    volumes:
      # Model storage
      - ollama_models:/root/.ollama
    # No GPU configuration for macOS
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - deepscrape-network

  # Ollama model loader - pulls default model on startup
  ollama-loader:
    image: ollama/ollama:latest
    container_name: deepscrape-ollama-loader
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - DEFAULT_MODEL=${LLM_MODEL:-qwen3:latest}
    volumes:
      - ollama_models:/root/.ollama
    command: >
      sh -c "
        echo 'Waiting for Ollama to be ready...';
        sleep 10;
        echo 'Pulling model: ${DEFAULT_MODEL}';
        ollama pull ${DEFAULT_MODEL};
        echo 'Model ${DEFAULT_MODEL} pulled successfully';
      "
    networks:
      - deepscrape-network

volumes:
  ollama_models:
    driver: local

networks:
  deepscrape-network:
    external: true
    name: deepscrape-network